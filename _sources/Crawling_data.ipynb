{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling data PTA Trunojoyo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah pertama adalah import library scrapy yang nantinya akan digunakan untuk mengambil link dari halaman PTA yang akan kita ambil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class Url(scrapy.Spider):\n",
    "    name = \"url\"\n",
    "    start_urls = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        url = 'https://pta.trunojoyo.ac.id/c_search/byprod/7/'\n",
    "        for page in range(1,3):\n",
    "            self.start_urls.append(url + str(page))\n",
    "        \n",
    "    def parse(self, response):\n",
    "        for page in range(1,6):\n",
    "            for url in response.css('#content_journal > ul'):\n",
    "                yield {\n",
    "                    'url' : url.css('li:nth-child('+str(page)+') > div:nth-child(3) > a ::attr(href)').extract()                    \n",
    "                }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code tersebut digunakan untuk mengakses dan mengekstrak link dan mengambil link tersebut dan dijadikan 1 file dengan eksistensi .json<br>\n",
    "Dengan cara input command pada Terminal : scrapy runspider /namafile.py -o namafile.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Mengambil data abstrak dari web PTA Trunojoyo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "untuk mengambil data abstrak yang akan kita olah, kita membutuhkan library json untuk membuka dan membaca file.json yang berisi link halaman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import json\n",
    "\n",
    "class Pta(scrapy.Spider):\n",
    "    name = \"crawling\"\n",
    "    file_json = open(\"urls.json\")\n",
    "    start_urls = json.loads(file_json.read())\n",
    "    urls = []\n",
    "\n",
    "    for i in range(len(start_urls)):\n",
    "        b = start_urls[i]['url'][0]\n",
    "        urls.append(b)\n",
    "    \n",
    "    def start_requests(self):\n",
    "        for url in self.urls:\n",
    "            yield scrapy.Request(url = url, callback = self.parse)\n",
    "        \n",
    "    def parse(self, response):\n",
    "        # print(response.url)\n",
    "\n",
    "        for jurnal in response.css('#content_journal > ul > li'):\n",
    "            yield {\n",
    "                'Judul':jurnal.css('li > div:nth-child(2) > a::text').get(),\n",
    "                'Penulis':jurnal.css('li > div:nth-child(2) > div:nth-child(2) > span::text').get(),\n",
    "                'Dosbing_1':jurnal.css('li > div:nth-child(2) > div:nth-child(3) > span::text').get(),\n",
    "                'Dosbing_2':jurnal.css('li > div:nth-child(2) > div:nth-child(4) > span::text').get(),\n",
    "                'Abstrak_indo':jurnal.css('li > div:nth-child(4) > div:nth-child(2) > p::text').get(),\n",
    "                'Abstrak_english':jurnal.css('li > div:nth-child(4) > div:nth-child(4) > p::text').get()\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada fungsi parse, kita melakukan pengambilan data Abstrak Indo dengan cara mengcopy selector yang ada di bagian Abtrak Indo pada website PTA Trunojoyo.<br>\n",
    "Abstrak Indo tersebut kita beri nama fitur \"Contents\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
